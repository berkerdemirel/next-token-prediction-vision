defaults:
  - model: gpt2_small
  - data: mnist
  - lit_model: default

# --- General --- #
seed: 42

# --- Trainer --- #
trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 10
  accelerator: "gpu"
  devices: 1
  precision: 16-mixed # AMP / fp16
  gradient_clip_val: 1.0
  # accumulate_grad_batches can stay at 1; raise it if you hit VRAM limits
  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    project: "next-token-prediction-vision"
    name: "${data.name}-${model.name}"
  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: "checkpoints/${data.name}/${model.name}"
      filename: "{epoch:02d}-{val_loss:.2f}"
      save_top_k: 1
      monitor: "val_loss"
      mode: "min"

# --- Lightning Model --- #
lit_model:
  _target_: src.lit_model.LitGPT
  lr: 1e-4
  use_vqvae: true
  vqvae_path: "CompVis/ldm-celebahq-256"
  model_config: ${model}

# --- Checkpointing --- #
# checkpoint:
#   _target_: pytorch_lightning.callbacks.ModelCheckpoint
#   dirpath: "checkpoints/${data.name}/${model.name}"
#   filename: "{epoch:02d}-{val_loss:.2f}"
#   save_top_k: 1
#   monitor: "val_loss"
#   mode: "min"
